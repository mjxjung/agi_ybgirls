{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/yiji/Desktop/ybigta/project/í•´ì»¤í†¤/agi_ybgirls/agi_ybgirls/lib/python3.11/site-packages (from beautifulsoup4) (4.13.0)\n",
      "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.3 soupsieve-2.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json íŒŒì¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ ìµœìƒìœ„ keys: dict_keys(['api', 'content', 'elements', 'merged_elements', 'model', 'ocr', 'usage'])\n",
      "ğŸ”‘ content keys: dict_keys(['html', 'markdown', 'text'])\n",
      "api: <class 'str'>\n",
      "content: <class 'dict'>\n",
      "  â””â”€ html: <class 'str'>\n",
      "  â””â”€ markdown: <class 'str'>\n",
      "  â””â”€ text: <class 'str'>\n",
      "elements: <class 'list'>\n",
      "merged_elements: <class 'list'>\n",
      "model: <class 'str'>\n",
      "ocr: <class 'bool'>\n",
      "usage: <class 'dict'>\n",
      "  â””â”€ pages: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../Database/input/êµ¬ê°•_êµ¬ê°œì—´_crop_parsing.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ìµœìƒìœ„ í‚¤ í™•ì¸\n",
    "print(\"ğŸ”‘ ìµœìƒìœ„ keys:\", data.keys())\n",
    "\n",
    "# content ë‚´ë¶€ í‚¤\n",
    "if 'content' in data:\n",
    "    print(\"ğŸ”‘ content keys:\", data['content'].keys())\n",
    "\n",
    "# content ì•ˆì— 'elements'ê°€ ìˆë‹¤ë©´, ì²« ë²ˆì§¸ ìš”ì†Œ êµ¬ì¡° í™•ì¸\n",
    "if 'elements' in data['content']:\n",
    "    print(\"ğŸ“Œ elements[0] ì˜ˆì‹œ:\")\n",
    "    print(data['content']['elements'][0].keys())\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(f\"{k}: {type(v)}\")\n",
    "    if isinstance(v, dict):\n",
    "        for kk in v.keys():\n",
    "            print(f\"  â””â”€ {kk}: {type(v[kk])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë³‘ëª… ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…ë³‘ëª…: êµ¬ê°œì—´\n"
     ]
    }
   ],
   "source": [
    "html = data['content']['html']\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# ë³‘ëª… ì¶”ì¶œ (h1 ì—†ì„ ë•Œ ëŒ€ë¹„)\n",
    "disease_tag = soup.find(\"h1\")\n",
    "\n",
    "if disease_tag is None:\n",
    "    print(\"[âš ï¸ê²½ê³ ] h1 íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # h1ì´ ì—†ìœ¼ë©´ font-size:22pxì¸ ì²« <p>ë‚˜ <header>ë¥¼ ì°¾ì•„ì„œ ë³‘ëª… ì¶”ì¶œ\n",
    "    for tag in soup.find_all([\"p\", \"header\"]):\n",
    "        style = tag.get(\"style\", \"\")\n",
    "        if \"font-size:22px\" in style:\n",
    "            disease_tag = tag\n",
    "            print(\"[âš ï¸] fontsize 22 íƒœê·¸ë¡œ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "            break\n",
    "\n",
    "# ë³‘ëª… ìµœì¢… ì¶”ì¶œ\n",
    "if disease_tag:\n",
    "  disease_name = disease_tag.get_text(strip=True)\n",
    "else:\n",
    "  disease_name = \"Unknown Disease\"\n",
    "  print(\"[âš ï¸ê²½ê³ ] ë³‘ëª… ì¶”ì¶œ ì‹¤íŒ¨: h1 ë˜ëŠ” font-size:22px íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ 'â€¢ ì½˜í…ì¸ ëª… :' í¬í•¨ëœ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ\n",
    "if not disease_name:\n",
    "    for tag in soup.find_all(\"p\"):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if \"â€¢ ì½˜í…ì¸ ëª…\" in text:\n",
    "            # \"â€¢ ì½˜í…ì¸ ëª… : êµ¬ê°œì—´\" í˜•ì‹ì—ì„œ ì˜¤ë¥¸ìª½ ê°’ë§Œ ì¶”ì¶œ\n",
    "            match = re.search(r\"ì½˜í…ì¸ ëª…\\s*:\\s*(.+)\", text)\n",
    "            if match:\n",
    "                disease_name = match.group(1).strip()\n",
    "                break\n",
    "            \n",
    "print(f'âœ…ë³‘ëª…: {disease_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë‚´ìš© ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "current_section = None\n",
    "current_text = []\n",
    "\n",
    "first_page_font_threshold = 22\n",
    "other_page_font_threshold = 18\n",
    "\n",
    "for elem in data['elements']:\n",
    "    html = elem.get(\"content\", {}).get(\"html\", \"\")\n",
    "    text = BeautifulSoup(html, \"html.parser\").get_text(strip=True)\n",
    "    page = elem.get(\"page\", 1)\n",
    "    category = elem.get(\"category\", \"\")\n",
    "    style = \"\"\n",
    "    \n",
    "    # style ì •ë³´ ì¶”ì¶œ\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tag = soup.find()\n",
    "    if tag:\n",
    "        style = tag.get(\"style\", \"\")\n",
    "\n",
    "    # font-size ì¶”ì¶œ\n",
    "    font_size = 0\n",
    "    if \"font-size:\" in style:\n",
    "        try:\n",
    "            font_size = int(style.split(\"font-size:\")[1].replace(\"px\", \"\").strip())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # í˜ì´ì§€ë³„ ê¸°ì¤€ ì ìš©\n",
    "    threshold = first_page_font_threshold if page == 1 else other_page_font_threshold\n",
    "    is_heading = (category.startswith(\"heading\") or font_size >= threshold)\n",
    "\n",
    "    if not text:\n",
    "        continue\n",
    "\n",
    "    if is_heading:\n",
    "        if current_section and current_text:\n",
    "            chunks.append({\n",
    "                \"disease\": disease_name,\n",
    "                \"section\": current_section,\n",
    "                \"content\": \"\\n\".join(current_text),\n",
    "                \"page\": page\n",
    "            })\n",
    "            current_text = []\n",
    "\n",
    "        current_section = text\n",
    "    else:\n",
    "        current_text.append(text)\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥\n",
    "if current_section and current_text:\n",
    "    chunks.append({\n",
    "        \"disease\": disease_name,\n",
    "        \"section\": current_section,\n",
    "        \"content\": \"\\n\".join(current_text),\n",
    "        \"page\": page\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jsonìœ¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "print(\"done!\")\n",
    "\n",
    "# ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ê°€ë…ì„± ìˆê²Œ)\n",
    "with open(\"../../Database/output/processed/êµ¬ê°œì—´_crop_test.json\", \"w\", encoding=\"utf-8\") as f_json:\n",
    "    json.dump(chunks, f_json, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ë˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì €ì¥\n",
    "with open(\"../../Database/output/processed/êµ¬ê°œì—´_crop_test.txt\", \"w\", encoding=\"utf-8\") as f_txt:\n",
    "    for chunk in chunks:\n",
    "        f_txt.write(f\"[{chunk['section']}]\\n\")\n",
    "        f_txt.write(chunk[\"content\"] + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agi_ybgirls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
